{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''This is cs231 assignment3, training a good LSTM model using minpy'''\n",
    "\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from cs231n.image_utils import image_from_url\n",
    "import minpy.numpy as mp\n",
    "from minpy.core import convert_args, grad_and_loss, minpy_to_numpy\n",
    "from cs231n.rnn_layers_minpy import *\n",
    "from cs231n import optim_minpy as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CaptioningRNN(object):\n",
    "  \"\"\"\n",
    "  A CaptioningRNN produces captions from image features using a recurrent\n",
    "  neural network.\n",
    "  The RNN receives input vectors of size D, has a vocab size of V, works on\n",
    "  sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
    "  of dimension W, and operates on minibatches of size N.\n",
    "  Note that we don't use any regularization for the CaptioningRNN.\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, word_to_idx, input_dim=512, wordvec_dim=128,\n",
    "               hidden_dim=128, cell_type='lstm', dtype=None):\n",
    "    \"\"\"\n",
    "    Construct a new CaptioningRNN instance.\n",
    "    Inputs:\n",
    "    - word_to_idx: A dictionary giving the vocabulary. It contains V entries,\n",
    "      and maps each string to a unique integer in the range [0, V).\n",
    "    - input_dim: Dimension D of input image feature vectors.\n",
    "    - wordvec_dim: Dimension W of word vectors.\n",
    "    - hidden_dim: Dimension H for the hidden state of the RNN.\n",
    "    - cell_type: What type of RNN to use; either 'rnn' or 'l#stm'.\n",
    "    - dtype: numpy datatype to use; use float32 for training and float64 for\n",
    "      numeric gradient checking.\n",
    "    \"\"\"\n",
    "    if cell_type not in {'rnn', 'lstm'}:\n",
    "      raise ValueError('Invalid cell_type \"%s\"' % cell_type)\n",
    "    \n",
    "    self.cell_type = cell_type\n",
    "    self.dtype = dtype\n",
    "    self.word_to_idx = word_to_idx\n",
    "    self.idx_to_word = {i: w for w, i in word_to_idx.iteritems()}\n",
    "    self.params = {}\n",
    "    \n",
    "    vocab_size = len(word_to_idx)\n",
    "\n",
    "    self._null = word_to_idx['<NULL>']\n",
    "    self._start = word_to_idx.get('<START>', None)\n",
    "    self._end = word_to_idx.get('<END>', None)\n",
    "    \n",
    "    # Initialize word vectors\n",
    "    self.params['W_embed'] = np.random.randn(vocab_size, wordvec_dim)\n",
    "    self.params['W_embed'] /= 100\n",
    "    \n",
    "    # Initialize CNN -> hidden state projection parameters\n",
    "    self.params['W_proj'] = np.random.randn(input_dim, hidden_dim)\n",
    "    self.params['W_proj'] /= np.sqrt(input_dim)\n",
    "    self.params['b_proj'] = np.zeros(hidden_dim)\n",
    "\n",
    "    # Initialize parameters for the RNN\n",
    "    dim_mul = {'lstm': 4, 'rnn': 1}[cell_type]\n",
    "    self.params['Wx'] = np.random.randn(wordvec_dim, dim_mul * hidden_dim)\n",
    "    self.params['Wx'] /= np.sqrt(wordvec_dim)\n",
    "    self.params['Wh'] = np.random.randn(hidden_dim, dim_mul * hidden_dim)\n",
    "    self.params['Wh'] /= np.sqrt(hidden_dim)\n",
    "    self.params['b'] = np.zeros(dim_mul * hidden_dim)\n",
    "    \n",
    "    # Initialize output to vocab weights\n",
    "    self.params['W_vocab'] = np.random.randn(hidden_dim, vocab_size)\n",
    "    self.params['W_vocab'] /= np.sqrt(hidden_dim)\n",
    "    self.params['b_vocab'] = np.zeros(vocab_size)\n",
    "    \n",
    "    # TODO: Support unified type casting among cpu and gpu\n",
    "    # # Cast parameters to correct dtype\n",
    "    # for k, v in self.params.iteritems():\n",
    "    #   self.params[k] = v.astype(self.dtype)\n",
    "\n",
    "  \n",
    "  @convert_args\n",
    "  def rnnNet(self, W_proj, b_proj, embed, Wx, Wh, b, W_vocab, b_vocab, features, \n",
    "             captions_in, captions_out, mask):\n",
    "    # (1) Use an affine transformation to compute the initial hidden state     #\n",
    "    #     from the image features. This should produce an array of shape (N, H)#\n",
    "    h0 = affine_forward(features, W_proj, b_proj)\n",
    "    # (2) Use a word embedding layer to transform the words in captions_in     #\n",
    "    #     from indices to vectors, giving an array of shape (N, T, W).         #\n",
    "    #embed = word_embedding_forward(captions_in, W_embed)\n",
    "    # (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to    #\n",
    "    #     process the sequence of input word vectors and produce hidden state  #\n",
    "    #     vectors for all timesteps, producing an array of shape (N, T, H).    #\n",
    "    if self.cell_type == 'rnn':\n",
    "      rnn_out = rnn_forward(embed, h0, Wx, Wh, b)\n",
    "    else:\n",
    "      rnn_out = lstm_forward(embed, h0, Wx, Wh, b)\n",
    "    # (4) Use a (temporal) affine transformation to compute scores over the    #\n",
    "    #     vocabulary at every timestep using the hidden states, giving an      #\n",
    "    #     array of shape (N, T, V).                                            #\n",
    "    affine_out = temporal_affine_forward(rnn_out, W_vocab, b_vocab)\n",
    "    # (5) Use (temporal) softmax to compute loss using captions_out, ignoring  #\n",
    "    #     the points where the output word is <NULL> using the mask above.     #\n",
    "    loss = temporal_softmax_loss(affine_out, captions_out, mask)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "  @convert_args\n",
    "  def loss(self, features, captions):\n",
    "    \"\"\"\n",
    "    Compute training-time loss for the RNN. We input image features and\n",
    "    ground-truth captions for those images, and use an RNN (or LSTM) to compute\n",
    "    loss and gradients on all parameters.\n",
    "    \n",
    "    Inputs:\n",
    "    - features: Input image features, of shape (N, D)\n",
    "    - captions: Ground-truth captions; an integer array of shape (N, T) where\n",
    "      each element is in the range 0 <= y[i, t] < V\n",
    "      \n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar loss\n",
    "    - grads: Dictionary of gradients parallel to self.params\n",
    "    \"\"\"\n",
    "    # Cut captions into two pieces: captions_in has everything but the last word\n",
    "    # and will be input to the RNN; captions_out has everything but the first\n",
    "    # word and this is what we will expect the RNN to generate. These are offset\n",
    "    # by one relative to each other because the RNN should produce word (t+1)\n",
    "    # after receiving word t. The first element of captions_in will be the START\n",
    "    # token, and the first element of captions_out will be the first word.\n",
    "    captions_in = captions[:, :-1]\n",
    "    captions_out = captions[:, 1:]\n",
    "\n",
    "    # You'll need this \n",
    "    mask = (captions_out != self._null)\n",
    "\n",
    "    # Weight and bias for the affine transform from image features to initial\n",
    "    # hidden state\n",
    "    W_proj, b_proj = self.params['W_proj'], self.params['b_proj']\n",
    "    \n",
    "    # Word embedding matrix\n",
    "    W_embed = self.params['W_embed']\n",
    "    \n",
    "    # Input-to-hidden, hidden-to-hidden, and biases for the RNN\n",
    "    Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b']\n",
    "\n",
    "    # Weight and bias for the hidden-to-vocab transformation.\n",
    "    W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab']\n",
    "    \n",
    "    loss, grads = 0.0, {}\n",
    "    \n",
    "    embed = word_embedding_forward(captions_in, W_embed)\n",
    "    \n",
    "    grad_function = grad_and_loss(self.rnnNet, xrange(8))\n",
    "    grad_array, loss = grad_function(W_proj, b_proj, embed, Wx, Wh, b, W_vocab, b_vocab,\n",
    "                    features, captions_in, captions_out, mask) \n",
    "    \n",
    "    #                                                                          #\n",
    "    # In the backward pass you will need to compute the gradient of the loss   #\n",
    "    # with respect to all model parameters. Use the loss and grads variables   #\n",
    "    # defined above to store loss and gradients; grads[k] should give the      #\n",
    "    # gradients for self.params[k]. \n",
    "    ''' \n",
    "    grads['W_proj'] = grad_array[0]\n",
    "    grads['b_proj'] = grad_array[1]\n",
    "    grads['W_embed'] = grad_array[2]\n",
    "    grads['Wx'] = grad_array[3]\n",
    "    grads['Wh'] = grad_array[4]\n",
    "    grads['b'] = grad_array[5]\n",
    "    grads['W_vocab'] = grad_array[6]\n",
    "    grads['b_vocab'] = grad_array[7]\n",
    "    '''\n",
    "\n",
    "    grads['W_proj'] = grad_array[0]\n",
    "    grads['b_proj'] = grad_array[1]\n",
    "    grads['W_embed'] = word_embedding_backward(grad_array[2], captions_in, W_embed)\n",
    "    grads['Wx'] = grad_array[3]\n",
    "    grads['Wh'] = grad_array[4]\n",
    "    grads['b'] = grad_array[5]\n",
    "    grads['W_vocab'] = grad_array[6]\n",
    "    grads['b_vocab'] = grad_array[7]\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "  def sample(self, features, max_length=30):\n",
    "    \"\"\"\n",
    "    Run a test-time forward pass for the model, sampling captions for input\n",
    "    feature vectors.\n",
    "    At each timestep, we embed the current word, pass it and the previous hidden\n",
    "    state to the RNN to get the next hidden state, use the hidden state to get\n",
    "    scores for all vocab words, and choose the word with the highest score as\n",
    "    the next word. The initial hidden state is computed by applying an affine\n",
    "    transform to the input image features, and the initial word is the <START>\n",
    "    token.\n",
    "    For LSTMs you will also have to keep track of the cell state; in that case\n",
    "    the initial cell state should be zero.\n",
    "    Inputs:\n",
    "    - features: Array of input image features of shape (N, D).\n",
    "    - max_length: Maximum length T of generated captions.\n",
    "    Returns:\n",
    "    - captions: Array of shape (N, max_length) giving sampled captions,\n",
    "      where each element is an integer in the range [0, V). The first element\n",
    "      of captions should be the first sampled word, not the <START> token.\n",
    "    \"\"\"\n",
    "    N = features.shape[0]\n",
    "    #captions = self._null * np.ones((N, max_length), dtype=np.int32)\n",
    "    captions = self._null * np.ones((N, max_length), dtype=int)\n",
    "\n",
    "    # Unpack parameters\n",
    "    W_proj, b_proj = self.params['W_proj'], self.params['b_proj']\n",
    "    W_embed = self.params['W_embed']\n",
    "    Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b']\n",
    "    W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab']\n",
    "    \n",
    "    h = affine_forward(features, W_proj, b_proj)\n",
    "\n",
    "    if self.cell_type == 'lstm':\n",
    "      c = np.zeros(h.shape)\n",
    "\n",
    "    embed = self._start * np.ones(N, dtype=int)\n",
    "\n",
    "    for t in xrange(max_length):\n",
    "      # (1) Embed the previous word using the learned word embeddings\n",
    "      embed = word_embedding_forward(embed, W_embed)\n",
    "      # (2) Make an RNN / LSTM step using the previous hidden state and the\n",
    "      #      embedded current word to get the next hidden state.\n",
    "      if self.cell_type == 'rnn':\n",
    "        h = rnn_step_forward(embed, h, Wx, Wh, b)\n",
    "      else:\n",
    "        h, c = lstm_step_forward(embed, h, c, Wx, Wh, b)\n",
    "      # (3) Apply the learned affine transformation to the next hidden state to\n",
    "      #     get scores for all words in the vocabulary\n",
    "      out = affine_forward(h, W_vocab, b_vocab)\n",
    "\n",
    "      # (4) Select the word with the highest score as the next word, writing it\n",
    "      #     to the appropriate slot in the captions variable  \n",
    "      #x = out.argmax(axis=1)\n",
    "      embed = np.argmax(out, axis=1)\n",
    "\n",
    "      captions[:, t] = embed\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "    return captions\n",
    "\n",
    "\n",
    "class CaptioningSolver(object):\n",
    "  \"\"\"\n",
    "  A CaptioningSolver encapsulates all the logic necessary for training\n",
    "  image captioning models. The CaptioningSolver performs stochastic gradient\n",
    "  descent using different update rules defined in optim.py.\n",
    "  The solver accepts both training and validataion data and labels so it can\n",
    "  periodically check classification accuracy on both training and validation\n",
    "  data to watch out for overfitting.\n",
    "  To train a model, you will first construct a CaptioningSolver instance,\n",
    "  passing the model, dataset, and various options (learning rate, batch size,\n",
    "  etc) to the constructor. You will then call the train() method to run the \n",
    "  optimization procedure and train the model.\n",
    "  \n",
    "  After the train() method returns, model.params will contain the parameters\n",
    "  that performed best on the validation set over the course of training.\n",
    "  In addition, the instance variable solver.loss_history will contain a list\n",
    "  of all losses encountered during training and the instance variables\n",
    "  solver.train_acc_history and solver.val_acc_history will be lists containing\n",
    "  the accuracies of the model on the training and validation set at each epoch.\n",
    "  \n",
    "  Example usage might look something like this:\n",
    "  \n",
    "  data = load_coco_data()\n",
    "  model = MyAwesomeModel(hidden_dim=100)\n",
    "  solver = CaptioningSolver(model, data,\n",
    "                  update_rule='sgd',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  lr_decay=0.95,\n",
    "                  num_epochs=10, batch_size=100,\n",
    "                  print_every=100)\n",
    "  solver.train()\n",
    "  A CaptioningSolver works on a model object that must conform to the following\n",
    "  API:\n",
    "  - model.params must be a dictionary mapping string parameter names to numpy\n",
    "    arrays containing parameter values.\n",
    "  - model.loss(features, captions) must be a function that computes\n",
    "    training-time loss and gradients, with the following inputs and outputs:\n",
    "    Inputs:\n",
    "    - features: Array giving a minibatch of features for images, of shape (N, D\n",
    "    - captions: Array of captions for those images, of shape (N, T) where\n",
    "      each element is in the range (0, V].\n",
    "    Returns:\n",
    "    - loss: Scalar giving the loss\n",
    "    - grads: Dictionary with the same keys as self.params mapping parameter\n",
    "      names to gradients of the loss with respect to those parameters.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, model, data, **kwargs):\n",
    "    \"\"\"\n",
    "    Construct a new CaptioningSolver instance.\n",
    "    \n",
    "    Required arguments:\n",
    "    - model: A model object conforming to the API described above\n",
    "    - data: A dictionary of training and validation data from load_coco_data\n",
    "    Optional arguments:\n",
    "    - update_rule: A string giving the name of an update rule in optim.py.\n",
    "      Default is 'sgd'.\n",
    "    - optim_config: A dictionary containing hyperparameters that will be\n",
    "      passed to the chosen update rule. Each update rule requires different\n",
    "      hyperparameters (see optim.py) but all update rules require a\n",
    "      'learning_rate' parameter so that should always be present.\n",
    "    - lr_decay: A scalar for learning rate decay; after each epoch the learning\n",
    "      rate is multiplied by this value.\n",
    "    - batch_size: Size of minibatches used to compute loss and gradient during\n",
    "      training.\n",
    "    - num_epochs: The number of epochs to run for during training.\n",
    "    - print_every: Integer; training losses will be printed every print_every\n",
    "      iterations.\n",
    "    - verbose: Boolean; if set to false then no output will be printed during\n",
    "      training.\n",
    "    \"\"\"\n",
    "    self.model = model\n",
    "    self.data = data\n",
    "    \n",
    "    # Unpack keyword arguments\n",
    "    self.update_rule = kwargs.pop('update_rule', 'sgd')\n",
    "    self.optim_config = kwargs.pop('optim_config', {})\n",
    "    self.lr_decay = kwargs.pop('lr_decay', 1.0)\n",
    "    self.batch_size = kwargs.pop('batch_size', 100)\n",
    "    self.num_epochs = kwargs.pop('num_epochs', 10)\n",
    "\n",
    "    self.print_every = kwargs.pop('print_every', 10)\n",
    "    self.verbose = kwargs.pop('verbose', True)\n",
    "\n",
    "    # Throw an error if there are extra keyword arguments\n",
    "    if len(kwargs) > 0:\n",
    "      extra = ', '.join('\"%s\"' % k for k in kwargs.keys())\n",
    "      raise ValueError('Unrecognized arguments %s' % extra)\n",
    "\n",
    "    # Make sure the update rule exists, then replace the string\n",
    "    # name with the actual function\n",
    "    if not hasattr(optim, self.update_rule):\n",
    "      raise ValueError('Invalid update_rule \"%s\"' % self.update_rule)\n",
    "    self.update_rule = getattr(optim, self.update_rule)\n",
    "\n",
    "    self._reset()\n",
    "\n",
    "\n",
    "  def _reset(self):\n",
    "    \"\"\"\n",
    "    Set up some book-keeping variables for optimization. Don't call this\n",
    "    manually.\n",
    "    \"\"\"\n",
    "    # Set up some variables for book-keeping\n",
    "    self.epoch = 0\n",
    "    self.best_val_acc = 0\n",
    "    self.best_params = {}\n",
    "    self.loss_history = []\n",
    "    self.train_acc_history = []\n",
    "    self.val_acc_history = []\n",
    "\n",
    "    # Make a deep copy of the optim_config for each parameter\n",
    "    self.optim_configs = {}\n",
    "    for p in self.model.params:\n",
    "      d = {k: v for k, v in self.optim_config.iteritems()}\n",
    "      self.optim_configs[p] = d\n",
    "\n",
    "\n",
    "  def _step(self):\n",
    "    \"\"\"\n",
    "    Make a single gradient update. This is called by train() and should not\n",
    "    be called manually.\n",
    "    \"\"\"\n",
    "    # Make a minibatch of training data\n",
    "    minibatch = sample_coco_minibatch(self.data,\n",
    "                  batch_size=self.batch_size,\n",
    "                  split='train')\n",
    "    captions, features, urls = minibatch\n",
    "\n",
    "    # Compute loss and gradient\n",
    "    loss, grads = self.model.loss(features, captions)\n",
    "    self.loss_history.append(loss)\n",
    "\n",
    "    # Perform a parameter update\n",
    "    for p, w in self.model.params.iteritems():\n",
    "      dw = grads[p]\n",
    "      config = self.optim_configs[p]\n",
    "      next_w, next_config = self.update_rule(w, dw, config)\n",
    "      self.model.params[p] = next_w\n",
    "      self.optim_configs[p] = next_config\n",
    "\n",
    "  def train(self):\n",
    "    \"\"\"\n",
    "    Run optimization to train the model.\n",
    "    \"\"\"\n",
    "    num_train = self.data['train_captions'].shape[0]\n",
    "    iterations_per_epoch = max(num_train / self.batch_size, 1)\n",
    "    num_iterations = self.num_epochs * iterations_per_epoch\n",
    "\n",
    "    for t in xrange(num_iterations):\n",
    "      self._step()\n",
    "\n",
    "      # Maybe print training loss\n",
    "      if self.verbose and t % self.print_every == 0:\n",
    "        print '(Iteration %d / %d) loss: %f' % (\n",
    "               t + 1, num_iterations, self.loss_history[-1])\n",
    "\n",
    "      # At the end of every epoch, increment the epoch counter and decay the\n",
    "      # learning rate.\n",
    "      epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "      if epoch_end:\n",
    "        self.epoch += 1\n",
    "        for k in self.optim_configs:\n",
    "          self.optim_configs[k]['learning_rate'] *= self.lr_decay\n",
    "\n",
    "      # Check train and val accuracy on the first iteration, the last\n",
    "      # iteration, and at the end of each epoch.\n",
    "      # TODO: Implement some logic to check Bleu on validation set periodically\n",
    "\n",
    "    # At the end of training swap the best params into the model\n",
    "# self.model.params = self.best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PrimitiveSelector' object has no attribute 'at'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-225a81796373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mlstm_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Time Elapsed:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-46231d34cbb0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m       \u001b[0;31m# Maybe print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-46231d34cbb0>\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;31m# Compute loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/minpy/core.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mmpy_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# call func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmpy_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmpy_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;31m# pylint: enable= missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-46231d34cbb0>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, features, captions)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W_proj'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b_proj'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W_embed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embedding_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wh'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/minpy/core.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mmpy_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# call func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmpy_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmpy_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;31m# pylint: enable= missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-46231d34cbb0>\u001b[0m in \u001b[0;36mword_embedding_backward\u001b[0;34m(dout, x, W)\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m##############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# a simple x in the second arg means indexing into 1-dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m   \u001b[0;31m##############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;31m#                               END OF YOUR CODE                             #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PrimitiveSelector' object has no attribute 'at'"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_dim = 512\n",
    "wordvec_dim = 256\n",
    "max_train = 1280\n",
    "learning_rate = 5e-3\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "\n",
    "data = load_coco_data(max_train=max_train)\n",
    "lstm_model = CaptioningRNN(\n",
    "          cell_type='lstm',\n",
    "          word_to_idx=data['word_to_idx'],\n",
    "          input_dim=data['train_features'].shape[1],\n",
    "          hidden_dim=512,\n",
    "          wordvec_dim=256,\n",
    "          dtype=np.float32,\n",
    "        )\n",
    "\n",
    "lstm_solver = CaptioningSolver(lstm_model, data,\n",
    "           update_rule='adam',\n",
    "           num_epochs=num_epochs,\n",
    "           batch_size=batch_size,\n",
    "           optim_config={\n",
    "             'learning_rate': learning_rate,\n",
    "           },\n",
    "           lr_decay=0.995,\n",
    "           verbose=True, print_every=10,\n",
    "         )\n",
    "import time\n",
    "start = time.time()\n",
    "lstm_solver.train()\n",
    "end = time.time()\n",
    "print 'Time Elapsed:', end - start\n",
    "\n",
    "# Plot the training losses\n",
    "plt.plot(lstm_solver.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "  minibatch = sample_coco_minibatch(data, split=split, batch_size=2)\n",
    "  gt_captions, features, urls = minibatch\n",
    "  gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "  sample_captions = minpy_to_numpy(lstm_model.sample(features))\n",
    "  sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "  for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "    plt.imshow(image_from_url(url))\n",
    "    plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
